{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.A. Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from -r ./Requirements.txt (line 1)) (2.21.0)\n",
      "Requirement already satisfied: feedly-client in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from -r ./Requirements.txt (line 2)) (0.19)\n",
      "Requirement already satisfied: pyquery in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from -r ./Requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from requests->-r ./Requirements.txt (line 1)) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from requests->-r ./Requirements.txt (line 1)) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from requests->-r ./Requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from requests->-r ./Requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: lxml>=2.1 in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from pyquery->-r ./Requirements.txt (line 3)) (4.2.5)\n",
      "Requirement already satisfied: cssselect>0.7.9 in /Users/clementpiat/anaconda3/lib/python3.7/site-packages (from pyquery->-r ./Requirements.txt (line 3)) (1.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r './Requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedly.session import FeedlySession\n",
    "from feedly.data import StreamOptions, Streamable\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from pyquery import PyQuery\n",
    "import re\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.B. First tests of the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = FeedlySession(auth=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal feeds:\n",
      "\t \"84ba8ab4-2a00-4af8-b574-3ed9e2f6a8d9\" (Random)\n",
      "\t \"aa5c0982-1aa4-4598-b9d5-fd2f17c697d2\" (AGI)\n",
      "\t \"d6be99dc-5a14-45f5-8d2a-9dd6937c6d93\" (Start-up)\n",
      "Personal boards:\n"
     ]
    }
   ],
   "source": [
    "# print the names of all your feeds (categories) and boards (tags)\n",
    "if(token):\n",
    "    print(\"Personal feeds:\")\n",
    "    for category_uuid, category_data in sess.user.get_categories().items():\n",
    "      print(\"\\t\", f'\"{category_uuid}\"', f\"({category_data['label']})\")\n",
    "    print(\"Personal boards:\")\n",
    "    for tag_name in sess.user.get_tags():\n",
    "      print(\"\\t\",f'\"{tag_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DOWNLOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.A. Retrieve artciles from the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_feed = \"feed/http://feeds.harvardbusiness.org/harvardbusiness/\"\n",
    "base_query = f\"/v3/streams/contents?streamId={source_feed}\" \n",
    "articles = list(Streamable({'id': source_feed}, sess).stream_contents(options=StreamOptions(max_count=k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1693b23473e:4d029:5280bd28',\n",
       " 'keywords': ['Technology', 'Innovation', 'Sponsor Content'],\n",
       " 'originId': 'tag:blogs.harvardbusiness.org,2007-03-31:999.223783',\n",
       " 'fingerprint': '92f159b',\n",
       " 'content': {'content': '<p>Sponsor content from Deloitte.</p>\\n<div>\\n<a href=\"http://feeds.harvardbusiness.org/~ff/harvardbusiness?a=n21kZld3DR0:MJzjIKqZRvU:yIl2AUoC8zA\"><img border=\"0\" src=\"http://feeds.feedburner.com/~ff/harvardbusiness?d=yIl2AUoC8zA\"></a> <a href=\"http://feeds.harvardbusiness.org/~ff/harvardbusiness?a=n21kZld3DR0:MJzjIKqZRvU:bcOpcFrp8Mo\"><img border=\"0\" src=\"http://feeds.feedburner.com/~ff/harvardbusiness?d=bcOpcFrp8Mo\"></a>\\n</div><img width=\"1\" alt=\"\" src=\"http://feeds.feedburner.com/~r/harvardbusiness/~4/n21kZld3DR0\" height=\"1\">',\n",
       "  'direction': 'ltr'},\n",
       " 'title': 'Cloud-Based Services Are Making It Easier for Companies to Use AI - SPONSOR CONTENT FROM DELOITTE',\n",
       " 'updated': 1551474841000,\n",
       " 'crawled': 1551475361598,\n",
       " 'published': 1551456343000,\n",
       " 'canonical': [{'href': 'https://hbr.org/sponsored/2019/03/cloud-based-services-are-making-it-easier-for-companies-to-use-ai',\n",
       "   'type': 'text/html'}],\n",
       " 'origin': {'streamId': 'feed/http://feeds.harvardbusiness.org/harvardbusiness/',\n",
       "  'title': 'Harvard Business Review ',\n",
       "  'htmlUrl': 'http://hbr.org'},\n",
       " 'alternate': [{'href': 'http://feeds.harvardbusiness.org/~r/harvardbusiness/~3/n21kZld3DR0/cloud-based-services-are-making-it-easier-for-companies-to-use-ai',\n",
       "   'type': 'text/html'}],\n",
       " 'visual': {'url': 'https://hbr.org/resources/images/article_assets/2019/02/deloitte.article10.image_.jpg',\n",
       "  'width': 700,\n",
       "  'height': 400,\n",
       "  'processor': 'feedly-nikon-v3.1',\n",
       "  'contentType': 'image/jpeg; charset=UTF-8'},\n",
       " 'unread': False,\n",
       " 'commonTopics': [{'id': 'nlp/f/topic/10001',\n",
       "   'label': 'artificial_intelligence'},\n",
       "  {'id': 'nlp/f/topic/56', 'label': 'ai'},\n",
       "  {'id': 'nlp/f/topic/1381', 'label': 'machine learning'},\n",
       "  {'id': 'nlp/f/topic/1585', 'label': 'nlp'},\n",
       "  {'id': 'nlp/f/topic/487', 'label': 'cloud'},\n",
       "  {'id': 'nlp/f/topic/253', 'label': 'big data'},\n",
       "  {'id': 'nlp/f/topic/2192', 'label': 'strategy'},\n",
       "  {'id': 'nlp/f/topic/1048', 'label': 'healthcare'},\n",
       "  {'id': 'nlp/f/topic/1931', 'label': 'research'},\n",
       "  {'id': 'nlp/f/topic/1398', 'label': 'management'},\n",
       "  {'id': 'nlp/f/topic/630', 'label': 'deep learning'}],\n",
       " 'entities': [{'id': 'nlp/f/entity/wd:491748',\n",
       "   'label': 'Deloitte',\n",
       "   'mentions': [{'text': 'Deloitte'}],\n",
       "   'salienceLevel': 'mention'},\n",
       "  {'id': 'nlp/f/entity/wd:941127',\n",
       "   'label': 'Salesforce.com',\n",
       "   'mentions': [{'text': 'Salesforce'}],\n",
       "   'salienceLevel': 'mention'},\n",
       "  {'id': 'nlp/f/entity/wd:937',\n",
       "   'label': 'Albert Einstein',\n",
       "   'mentions': [{'text': 'Einstein'}],\n",
       "   'salienceLevel': 'mention'},\n",
       "  {'id': 'nlp/f/entity/wd:4738155',\n",
       "   'label': 'financial management',\n",
       "   'mentions': [{'text': 'Financial management'}],\n",
       "   'salienceLevel': 'mention'}],\n",
       " 'engagement': 1385,\n",
       " 'webfeeds': {'relatedLayout': 'card',\n",
       "  'relatedTarget': 'browser',\n",
       "  'accentColor': '#0787b1',\n",
       "  'logo': 'http://storage.googleapis.com/test-site-assets/RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs_ologo-14d31722c5c',\n",
       "  'partial': True}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[199].json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) The other fields available are  ['id', 'keywords', 'originId', 'fingerprint', 'content', 'title', 'updated', 'crawled', 'published', 'canonical', 'origin', 'alternate', 'visual', 'unread', 'commonTopics', 'entities', 'engagement', 'webfeeds']\n"
     ]
    }
   ],
   "source": [
    "print(\"2) The other fields available are \",list(articles[199].json.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id','keywords','content','title','canonical','visual','commonTopics','engagement','published']\n",
    "l = len(columns)\n",
    "data = np.zeros((k,l),dtype = object)\n",
    "\n",
    "i=0\n",
    "for ar in articles :\n",
    "    for j in range(l):\n",
    "        try :\n",
    "            data[i,j] = ar.json[columns[j]]\n",
    "        except:\n",
    "            data[i,j] = 'impossible to get'\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I just store the 500 artciles in an array and then in a pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>canonical</th>\n",
       "      <th>visual</th>\n",
       "      <th>commonTopics</th>\n",
       "      <th>engagement</th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Economics &amp; Society, Managing organizations, ...</td>\n",
       "      <td>{'content': '&lt;p&gt;Youngme, Felix, and Mihir offe...</td>\n",
       "      <td>Quick Takes on Uber, Apple/Qualcomm, Pinterest...</td>\n",
       "      <td>[{'href': 'https://hbr.org/podcast/2019/05/qui...</td>\n",
       "      <td>{'edgeCacheUrl': 'https://lh3.googleuserconten...</td>\n",
       "      <td>impossible to get</td>\n",
       "      <td>5900</td>\n",
       "      <td>1556721691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Technology, Health, Innovation, Audio]</td>\n",
       "      <td>{'content': '&lt;p&gt;Eric Topol, the foremost exper...</td>\n",
       "      <td>When AI Meets Medicine</td>\n",
       "      <td>[{'href': 'https://hbr.org/podcast/2019/05/whe...</td>\n",
       "      <td>{'edgeCacheUrl': 'https://lh3.googleuserconten...</td>\n",
       "      <td>[{'id': 'nlp/f/topic/1734', 'label': 'pharma'}...</td>\n",
       "      <td>400</td>\n",
       "      <td>1556714915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Technology, Competitive strategy, Digital Art...</td>\n",
       "      <td>{'content': '&lt;p&gt;There are more risks than most...</td>\n",
       "      <td>Should Your Company’s Voice Strategy Be Based ...</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/05/should-your...</td>\n",
       "      <td>{'edgeCacheUrl': 'https://lh3.googleuserconten...</td>\n",
       "      <td>[{'id': 'nlp/f/topic/487', 'label': 'cloud'}, ...</td>\n",
       "      <td>5228</td>\n",
       "      <td>1556719231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Boards, Gender, Strategic planning, Digital A...</td>\n",
       "      <td>{'content': '&lt;p&gt;At the current rate, U.S. boar...</td>\n",
       "      <td>Boards Are Overlooking Qualified Women. Here’s...</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/05/boards-are-...</td>\n",
       "      <td>{'edgeCacheUrl': 'https://lh3.googleuserconten...</td>\n",
       "      <td>[{'id': 'nlp/f/topic/741', 'label': 'education...</td>\n",
       "      <td>5572</td>\n",
       "      <td>1556715627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Difficult conversations, Conflict, Digital Ar...</td>\n",
       "      <td>{'content': '&lt;p&gt;Swallow your pride and apologi...</td>\n",
       "      <td>What to Do After an Uncomfortable Conversation...</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/05/what-to-do-...</td>\n",
       "      <td>{'edgeCacheUrl': 'https://lh3.googleuserconten...</td>\n",
       "      <td>[{'id': 'nlp/f/topic/1309', 'label': 'leadersh...</td>\n",
       "      <td>5680</td>\n",
       "      <td>1556712349000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "1  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "2  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "3  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "4  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [Economics & Society, Managing organizations, ...   \n",
       "1            [Technology, Health, Innovation, Audio]   \n",
       "2  [Technology, Competitive strategy, Digital Art...   \n",
       "3  [Boards, Gender, Strategic planning, Digital A...   \n",
       "4  [Difficult conversations, Conflict, Digital Ar...   \n",
       "\n",
       "                                             content  \\\n",
       "0  {'content': '<p>Youngme, Felix, and Mihir offe...   \n",
       "1  {'content': '<p>Eric Topol, the foremost exper...   \n",
       "2  {'content': '<p>There are more risks than most...   \n",
       "3  {'content': '<p>At the current rate, U.S. boar...   \n",
       "4  {'content': '<p>Swallow your pride and apologi...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Quick Takes on Uber, Apple/Qualcomm, Pinterest...   \n",
       "1                             When AI Meets Medicine   \n",
       "2  Should Your Company’s Voice Strategy Be Based ...   \n",
       "3  Boards Are Overlooking Qualified Women. Here’s...   \n",
       "4  What to Do After an Uncomfortable Conversation...   \n",
       "\n",
       "                                           canonical  \\\n",
       "0  [{'href': 'https://hbr.org/podcast/2019/05/qui...   \n",
       "1  [{'href': 'https://hbr.org/podcast/2019/05/whe...   \n",
       "2  [{'href': 'https://hbr.org/2019/05/should-your...   \n",
       "3  [{'href': 'https://hbr.org/2019/05/boards-are-...   \n",
       "4  [{'href': 'https://hbr.org/2019/05/what-to-do-...   \n",
       "\n",
       "                                              visual  \\\n",
       "0  {'edgeCacheUrl': 'https://lh3.googleuserconten...   \n",
       "1  {'edgeCacheUrl': 'https://lh3.googleuserconten...   \n",
       "2  {'edgeCacheUrl': 'https://lh3.googleuserconten...   \n",
       "3  {'edgeCacheUrl': 'https://lh3.googleuserconten...   \n",
       "4  {'edgeCacheUrl': 'https://lh3.googleuserconten...   \n",
       "\n",
       "                                        commonTopics engagement      published  \n",
       "0                                  impossible to get       5900  1556721691000  \n",
       "1  [{'id': 'nlp/f/topic/1734', 'label': 'pharma'}...        400  1556714915000  \n",
       "2  [{'id': 'nlp/f/topic/487', 'label': 'cloud'}, ...       5228  1556719231000  \n",
       "3  [{'id': 'nlp/f/topic/741', 'label': 'education...       5572  1556715627000  \n",
       "4  [{'id': 'nlp/f/topic/1309', 'label': 'leadersh...       5680  1556712349000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data,columns=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.B. Retrieve complete articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticleFromUrl(url):\n",
    "    r = requests.get(url)\n",
    "    html = r.content\n",
    "    pq = PyQuery(html)\n",
    "    tag = pq('div.article')\n",
    "    return(tag.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's a function to get the complete article from href url given in the canonical field : I get the article with the request module and then I parse it with the PyQuery module (It's possible to parse it automatically because we retrieve the article from the same website HBR so the format is the same, i.e. the article content can always be found in the div with class name \"article\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:50<00:00,  2.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_16a73e74fa0:29de9:2a7e54a4',\n",
       "       list(['Economics & Society', 'Managing organizations', 'Operations', 'Audio']),\n",
       "       {'content': '<p>Youngme, Felix, and Mihir offer short takes on six different topics, including: Uber’s S-1, Apple and Qualcomm settling their lawsuit, Pinterest’s prospects as a public company, Brexit, and the best TV shows about business.</p>\\n<div>\\n<a href=\"http://feeds.harvardbusiness.org/~ff/harvardbusiness?a=zmHyLgO7OOE:wzh7H_eVEQo:yIl2AUoC8zA\"><img border=\"0\" src=\"http://feeds.feedburner.com/~ff/harvardbusiness?d=yIl2AUoC8zA\"></a> <a href=\"http://feeds.harvardbusiness.org/~ff/harvardbusiness?a=zmHyLgO7OOE:wzh7H_eVEQo:bcOpcFrp8Mo\"><img border=\"0\" src=\"http://feeds.feedburner.com/~ff/harvardbusiness?d=bcOpcFrp8Mo\"></a>\\n</div><img width=\"1\" alt=\"\" src=\"http://feeds.feedburner.com/~r/harvardbusiness/~4/zmHyLgO7OOE\" height=\"1\">', 'direction': 'ltr'},\n",
       "       'Quick Takes on Uber, Apple/Qualcomm, Pinterest, Brexit, and More',\n",
       "       list([{'href': 'https://hbr.org/podcast/2019/05/quick-takes-on-uber-apple-qualcomm-pinterest-brexit-and-more', 'type': 'text/html'}]),\n",
       "       {'edgeCacheUrl': 'https://lh3.googleusercontent.com/fO718IXrQAxzvM45MYVv_p85BaicOo2DLBhA_xKiZbzLikHrXu2ETKC7JPM8bn7cA_NU3H19Ee_lF9O6pzi6vui6s-0', 'processor': 'feedly-nikon-v3.1', 'url': 'https://hbr.org/resources/images/article_assets/2018/10/wide-after-hours.jpg', 'width': 1200, 'height': 675, 'expirationDate': 1559314714330, 'contentType': 'image/jpeg; charset=UTF-8'},\n",
       "       'impossible to get', 5900, 1556721691000,\n",
       "       'Listen and subscribe to this podcast via Apple Podcasts | Google Podcasts | RSS\\nYoungme Moon, Felix Oberholzer-Gee, and Mihir Desai offer short takes on six different topics, including: Uber’s S-1, Apple and Qualcomm settling their lawsuit, Pinterest’s prospects as a public company, Brexit, and the best TV shows about business.\\nDownload this podcast\\nSome recent picks:\\nSec.gov (to download S-1s)\\nBilly Eilish\\n#MidtownUniform\\nEater.com\\nCribsheet: A Data-Driven Guide to Better, More Relaxed Parenting (Emily Oster)\\nPitchfork (Music Recommendations)\\nHomecoming King (Hasan Minhaj on Netflix)\\nYou can email your comments and ideas for future episodes to: harvardafterhours@gmail.com. You can follow Youngme and Mihir on Twitter at: @YoungmeMoon and @DesaiMihirA.\\nHBR Presents is a network of podcasts curated by HBR editors, bringing you the best business ideas from the leading minds in management. The views and opinions expressed are solely those of the authors and do not necessarily reflect the official policy or position of Harvard Business Review or its affiliates.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 4 \n",
    "articlesContent = np.zeros((k,1),dtype = object)\n",
    "\n",
    "for i in tqdm(range(k)):\n",
    "    url = data[i,j][0]['href']\n",
    "    articlesContent[i,0] = getArticleFromUrl(url)\n",
    "\n",
    "data = np.concatenate((data,articlesContent),axis = 1)\n",
    "columns += ['content']\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "data[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.C. First processing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string,strip_multiple_whitespaces,strip_punctuation, strip_non_alphanum\n",
    "\n",
    "filters = [strip_punctuation,strip_multiple_whitespaces,strip_non_alphanum]\n",
    "\n",
    "firstTreatment = []\n",
    "\n",
    "for i in range(k):\n",
    "    firstTreatment.append(preprocess_string(data[i,-1],filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We transform the raw article in a list of tokens and apply some filters to retrieve punctuation, non alphanumerical characters and multiple whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) The ratio of the capitalized word to non­capitalized words is  10.52 %\n"
     ]
    }
   ],
   "source": [
    "compteur = 0\n",
    "upper = 0\n",
    "for i in range(k):\n",
    "    for word in firstTreatment[i]:\n",
    "        if word[0].isupper():\n",
    "            upper+=1\n",
    "        compteur+=1\n",
    "\n",
    "print(\"3) The ratio of the capitalized word to non­capitalized words is \",int(upper/compteur*10000)/100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = re.compile('\\d+')\n",
    "for i in range(k):\n",
    "    for j in range(len(firstTreatment[i])):\n",
    "        word = firstTreatment[i][j]\n",
    "        firstTreatment[i][j] = number.sub('isanumber', word).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe the number 50 is not important to know if an article is about leadership but maybe the fact that an article contains a certain amount of number in it can be meaningful, so we will just transform every number in the string \"isanumber\" to reduce the vocabulary while keeping useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((data,np.reshape(firstTreatment,(k,1))),axis = 1)\n",
    "columns += ['tokens']\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen', 'and', 'subscribe', 'to', 'this', 'podcast', 'via', 'apple', 'podcasts', 'google', 'podcasts', 'rss', 'youngme', 'moon', 'felix', 'oberholzer', 'gee', 'and', 'mihir', 'desai', 'offer', 'short', 'takes', 'on', 'six', 'different', 'topics', 'including', 'uber', 's', 's', 'isanumber', 'apple', 'and', 'qualcomm', 'settling', 'their', 'lawsuit', 'pinterest', 's', 'prospects', 'as', 'a', 'public', 'company', 'brexit', 'and', 'the', 'best', 'tv', 'shows', 'about', 'business', 'download', 'this', 'podcast', 'some', 'recent', 'picks', 'sec', 'gov', 'to', 'download', 's', 'isanumbers', 'billy', 'eilish', 'midtownuniform', 'eater', 'com', 'cribsheet', 'a', 'data', 'driven', 'guide', 'to', 'better', 'more', 'relaxed', 'parenting', 'emily', 'oster', 'pitchfork', 'music', 'recommendations', 'homecoming', 'king', 'hasan', 'minhaj', 'on', 'netflix', 'you', 'can', 'email', 'your', 'comments', 'and', 'ideas', 'for', 'future']\n"
     ]
    }
   ],
   "source": [
    "print(data[0,-1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dictionary(firstTreatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With that first preprocessing we get 20778 words in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(\"With that first preprocessing we get \"+str(len(d))+\" words in the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.556"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)/500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.D. Other preprocessing step to reduce the number of words by using stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import stem_text\n",
    "\n",
    "filters = [stem_text]\n",
    "\n",
    "secondTreatment = []\n",
    "\n",
    "for i in range(k):\n",
    "    secondTreatment.append([])\n",
    "    for word in firstTreatment[i]:\n",
    "        secondTreatment[i].append(preprocess_string(word,filters)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen', 'and', 'subscrib', 'to', 'thi', 'podcast', 'via', 'appl', 'podcast', 'googl', 'podcast', 'rss', 'youngm', 'moon', 'felix', 'oberholz', 'gee', 'and', 'mihir', 'desai', 'offer', 'short', 'take', 'on', 'six', 'differ', 'topic', 'includ', 'uber', 's', 's', 'isanumb', 'appl', 'and', 'qualcomm', 'settl', 'their', 'lawsuit', 'pinterest', 's', 'prospect', 'as', 'a', 'public', 'compani', 'brexit', 'and', 'the', 'best', 'tv', 'show', 'about', 'busi', 'download', 'thi', 'podcast', 'some', 'recent', 'pick', 'sec', 'gov', 'to', 'download', 's', 'isanumb', 'billi', 'eilish', 'midtownuniform', 'eater', 'com', 'cribsheet', 'a', 'data', 'driven', 'guid', 'to', 'better', 'more', 'relax', 'parent', 'emili', 'oster', 'pitchfork', 'music', 'recommend', 'homecom', 'king', 'hasan', 'minhaj', 'on', 'netflix', 'you', 'can', 'email', 'your', 'comment', 'and', 'idea', 'for', 'futur']\n"
     ]
    }
   ],
   "source": [
    "print(secondTreatment[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = Dictionary(secondTreatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now get a vocabulary of size  12549\n"
     ]
    }
   ],
   "source": [
    "print(\"We now get a vocabulary of size \",len(d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We divided the vocabulary size by almsot 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((data,np.reshape(secondTreatment,(k,1))),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n",
      "147\n",
      "149\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    \n",
    "    if(data[i,-3]==\"\"):\n",
    "        print(i)\n",
    "        data[i,0] = \"toDelete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(data[:,0]==\"toDelete\")]\n",
    "k = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We just remove rows with no complete articles (probably the http request which failed at the beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns += ['tokensWithStemming']\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "df.to_csv('dataBase.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.E. Analyse words frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataBase.csv')\n",
    "del(df['Unnamed: 0'])\n",
    "tab = np.array(df,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringToList(s):\n",
    "    l = s.split(\"', '\")\n",
    "    l[0] = l[0][2:]\n",
    "    l[-1] = l[-1][:-2]\n",
    "    return(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    tab[i,-1] = stringToList(tab[i,-1])\n",
    "    tab[i,-2] = stringToList(tab[i,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After passing threw the Dataframe list of tokens were converted into string so we had to reverse tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "vocab = list(d.values())\n",
    "for word in vocab:\n",
    "    frequency[word] = 0\n",
    "for i in range(k):\n",
    "    for word in tab[i,-2]:\n",
    "        frequency[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = sorted(frequency.items(), key=lambda x: x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 29829),\n",
       " ('to', 25399),\n",
       " ('and', 22313),\n",
       " ('of', 17544),\n",
       " ('a', 16239),\n",
       " ('that', 13942),\n",
       " ('in', 12132),\n",
       " ('you', 9376),\n",
       " ('is', 8012),\n",
       " ('s', 7855),\n",
       " ('it', 7820),\n",
       " ('i', 7514),\n",
       " ('for', 7238),\n",
       " ('with', 5232),\n",
       " ('are', 5128),\n",
       " ('they', 5127),\n",
       " ('we', 5116),\n",
       " ('isanumber', 4935),\n",
       " ('on', 4782),\n",
       " ('this', 4730)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We only get stopwords among the most frequent words... so we are going to remove them to get better insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "importantWords = []\n",
    "for i in range(300):\n",
    "    word = frequencies[i][0]\n",
    "    if(remove_stopwords(word) and len(word)>2):\n",
    "        importantWords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['isanumber', 'people', 'work', 'time', 'like', 'think', 'new', 'know', 'business', 'company', 'companies', 'data', 'team', 'need', 'way', 'want', 'women', 'going', 'employees', 'help', 'things', 'right', 'job', 'leaders', 'good', 'better', 'googletag', 'organization', 'research', 'feel', 'important', 'different', 'example', 'working', 'years', 'use', 'alison', 'change', 'performance', 'best', 'person', 'lot', 'managers', 'actually', 'kind', 'value', 'day', 'long', 'likely', 'beard', 'making', 'problem', 'said', 'organizations', 'high', 'management', 'customers', 'experience', 'world', 'ask', 'market', 'life', 'push', 'look', 'thing', 'great', 'information', 'function', 'having', 'sales', 'start', 'year', 'process', 'little', 'ways', 'boss', 'care', 'technology', 'says', 'come', 'point', 'strategy', 'hbr', 'images', 'create', 'let', 'based', 'digital', 'product', 'firms', 'employee', 'amy', 'able', 'role', 'questions', 'question', 'dan', 'big', 'teams', 'leadership', 'manager', 'getty', 'today', 'career', 'display', 'learning', 'understand', 'level', 'cmd', 'dfp', 'posisanumber', 'support', 'curt', 'future', 'yeah', 'try', 'case', 'talk', 'focus', 'thinking', 'nickisch', 'innovation', 'health', 'feedback', 'hard', 'needs', 'term', 'study', 'industry', 'customer', 'mean', 'impact', 'social', 'decision', 'home', 'group', 'self', 'ideas', 'set', 'getting', 'share', 'maybe']\n"
     ]
    }
   ],
   "source": [
    "print(importantWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('leadership' in importantWords)\n",
    "print('leader' in importantWords)\n",
    "print('lead' in importantWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create rule-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataBase.csv')\n",
    "del(df['Unnamed: 0'])\n",
    "data = np.array(df)\n",
    "columns = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtitle = columns.index(\"title\")\n",
    "def model1(document):\n",
    "    return ('leadership'in document[jtitle].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtokens = columns.index(\"tokens\")\n",
    "def model2(document):\n",
    "    return('leadership' in document[jtokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('lexvec.enwiki+newscrawl.300d.W.pos.vectors', binary=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To execute this cell you will have to got to https://github.com/alexandres/lexvec#pre-trained-vectors and download the vectors of size 398 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('frontbench', 0.540550947189331), ('leaders', 0.5190237164497375), ('leaderships', 0.4795929491519928), ('competence', 0.4795462191104889), ('governance', 0.47358646988868713), ('organizational', 0.4725119173526764), ('grassroots', 0.4708701968193054), ('party', 0.4704243838787079), ('unity', 0.4684489965438843), ('chairmanship', 0.46583208441734314)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['leadership'],topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In wikidpedia leadership appears in the same context as these words so we're going to try to include some of them in the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(document):\n",
    "    counter = 0\n",
    "    words = ['leadership','frontbench','leaders','leaderships','grassroots']\n",
    "    for word in words:\n",
    "        if((word in document[jtokens]) or (word in document[jtitle].lower())):\n",
    "            counter +=1\n",
    "    return counter>1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataBase.csv')\n",
    "del(df['Unnamed: 0'])\n",
    "data = np.array(df)\n",
    "columns = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = np.zeros((k,1))\n",
    "jkeywords = columns.index(\"keywords\")\n",
    "jtopics = columns.index(\"commonTopics\")\n",
    "for i in range(k):\n",
    "    if('leadership' in data[i,jkeywords].lower()):\n",
    "        classe[i]=1       \n",
    "    if('leadership' in data[i,jtopics].lower()):\n",
    "        classe[i]=1\n",
    "        \n",
    "dfclasse = pd.DataFrame(classe)\n",
    "dfclasse.to_csv('classe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 2 fields keywords and commonTopics which could correspond to \"tags\" so we will use both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is unbalanced, only 1/4 of the articles seems to be about leadership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataBase.csv')\n",
    "del(df['Unnamed: 0'])\n",
    "dfclasse = pd.read_csv('classe.csv')\n",
    "del(dfclasse['Unnamed: 0'])\n",
    "data = np.array(df)\n",
    "classe = np.array(dfclasse)\n",
    "columns = list(df.columns)\n",
    "for i in range(k):\n",
    "    data[i,-1] = stringToList(data[i,-1])\n",
    "    data[i,-2] = stringToList(data[i,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,model):\n",
    "    n,l = x.shape\n",
    "    predictions = []\n",
    "    for i in range(n):\n",
    "        predictions.append(model(x[i,:]))\n",
    "    return(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's just a function which returns the predictions of model on dataset x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "precision of model 1 : 74.39516129032258\n",
      "mean of predicted values : 0.012096774193548387\n",
      "--\n",
      "precision of model 2 : 60.88709677419355\n",
      "mean of predicted values : 0.28225806451612906\n",
      "--\n",
      "precision of model 3 : 64.11290322580645\n",
      "mean of predicted values : 0.21774193548387097\n"
     ]
    }
   ],
   "source": [
    "def evaluateModels(data,classe):\n",
    "    models = [model1,model2,model3]\n",
    "    for i in range(3):\n",
    "        ypredict = predict(data,models[i])\n",
    "        print('--')\n",
    "        print(\"precision of model \"+str(i+1)+\" :\", (1-np.mean(abs(ypredict-classe)))*100)\n",
    "        print(\"mean of predicted values :\",np.mean(ypredict) )\n",
    "\n",
    "evaluateModels(data,classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 seems to be better in term of accuracy but that's only because the dataset is unbalanced and this model almost never predict class 1 \n",
    "### Let's see what happends if we convert the dataset in a balanced one by deleting some class 0 samples for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "for i in range(k):\n",
    "    if(not classe[i]):\n",
    "        if(rd.random()>0.33):\n",
    "            data[i,0]='toDelete'\n",
    "            classe[i]=-1\n",
    "\n",
    "balancedData = data[~(data[:,0] == 'toDelete')]      \n",
    "balancedClasse = classe[~(classe[:]==-1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4901185770750988"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(balancedClasse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "precision of model 1 : 53.359683794466406\n",
      "mean of predicted values : 0.023715415019762844\n",
      "--\n",
      "precision of model 2 : 64.82213438735178\n",
      "mean of predicted values : 0.35968379446640314\n",
      "--\n",
      "precision of model 3 : 65.21739130434783\n",
      "mean of predicted values : 0.2924901185770751\n"
     ]
    }
   ],
   "source": [
    "evaluateModels(balancedData,balancedClasse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the best models seem to be the models based on the body of the article\n",
    "### So we can't conclude wich model is better, it actually depends on what is the metric considered which depends on the goal of the model / how we would use the model, they all have different false positive rate/ true positive rate/ false negative rate/ and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TFIDF - LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.A. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataBase.csv')\n",
    "del(df['Unnamed: 0'])\n",
    "dfclasse = pd.read_csv('classe.csv')\n",
    "del(dfclasse['Unnamed: 0'])\n",
    "data = np.array(df)\n",
    "classe = np.array(dfclasse)\n",
    "columns = list(df.columns)\n",
    "for i in range(k):\n",
    "    data[i,-1] = stringToList(data[i,-1])\n",
    "    data[i,-2] = stringToList(data[i,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = []\n",
    "for i in range(k):\n",
    "    X.append(' '.join(data[i,-1]))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.7,min_df=0.01)\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the tfidf vectorizer of sklearn and store the vectors in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 4336)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.B. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tsvd = TruncatedSVD(n_components = 496)\n",
    "X2 = tsvd.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n,l = X.shape\n",
    "pca = PCA(n_components=496)\n",
    "pca.fit(X)\n",
    "variances = pca.explained_variance_ratio_\n",
    "variancesSum = [np.sum(variances[:i]) for i in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(variancesSum)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=300)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.C. Evaluation of the first models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78       0.8        0.73737374 0.78787879 0.82653061]\n",
      "0.7863566274994847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold as SK\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "scores = cross_val_score(clf,X,classe.ravel(),cv=SK(shuffle=True,n_splits=5))\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76       0.77       0.76767677 0.7979798  0.78571429]\n",
      "0.7762741702741702\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "scores = cross_val_score(clf,X2,classe.ravel(),cv=SK(shuffle=True,n_splits=5))\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The supervised learning approach seems to perform slightly better than the rule based approchaed in terms of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79       0.8        0.78787879 0.72727273 0.82653061]\n",
      "0.7863364254792826\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=1,solver='liblinear',class_weight='balanced',max_iter=1000,C=1)\n",
    "scores = cross_val_score(clf,X,classe.ravel(),cv=SK(shuffle=True,n_splits=5))\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not significative improvment by tuning by hand the model (in terms of accuracy again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.D. Try to improve the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this part I just wanted to try if I could get better results by \"increasing the database size\" by splitting every article randomly in 5 parts : for every article seen as a group of words (the order doesn't matter since we use TF-IDF method) I split it in 5 groupes of words randomly chosen, thus I eventually get a database with 5 times more rows. I thought that maybe we can consider that the first part of an article about leadership is a small article about leadership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataBase.csv')\n",
    "del(df['Unnamed: 0'])\n",
    "dfclasse = pd.read_csv('classe.csv')\n",
    "del(dfclasse['Unnamed: 0'])\n",
    "data = np.array(df)\n",
    "classe = np.array(dfclasse)\n",
    "columns = list(df.columns)\n",
    "for i in range(k):\n",
    "    data[i,-1] = stringToList(data[i,-1])\n",
    "    data[i,-2] = stringToList(data[i,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformX5(X,classe):\n",
    "    X5 = []\n",
    "    y5 = []\n",
    "    k=len(X)\n",
    "    for i in range(k):\n",
    "        l = np.array(data[i,-1])\n",
    "        n = len(l)\n",
    "        indexes = [j for j in range(n)]\n",
    "        rd.shuffle(indexes)\n",
    "        naux = n//5\n",
    "        for j in range(5):\n",
    "            indexesaux = indexes[j*naux:(j+1)*naux]\n",
    "            laux = l[indexesaux]\n",
    "            X5.append(laux)\n",
    "            y5.append(classe[i])\n",
    "\n",
    "    return(X5,y5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That is the function which multiply the number of rows of the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5,classe5 = transformX5(X,classe)\n",
    "X = []\n",
    "for i in range(len(X5)):\n",
    "    X.append(' '.join(X5[i]))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.7,min_df=0.01)\n",
    "X = vectorizer.fit_transform(X)\n",
    "n,l = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.todense()\n",
    "pca = PCA(n_components=300)\n",
    "X = pca.fit_transform(X)\n",
    "classe5 = np.array(classe5).reshape((n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86491935 0.83870968 0.7983871  0.67943548 0.62298387]\n",
      "0.7608870967741936\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=1,solver='liblinear',class_weight='balanced',max_iter=1000,C=1)\n",
    "scores = cross_val_score(clf,X,classe5.ravel(),cv=SK(shuffle=False,n_splits=5))\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(X,classe5,test_size = 0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7620967741935484"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain,ytrain = sklearn.utils.shuffle(xtrain,ytrain)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "clf.fit(xtrain,ytrain)\n",
    "clf.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It doesn't seem to be so good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5151515151515151\n",
      "0.5757575757575758\n",
      "0.5959595959595959\n",
      "0.5959595959595959\n",
      "0.6565656565656566\n"
     ]
    }
   ],
   "source": [
    "for counter in range(1,6):\n",
    "    true = 0\n",
    "    predictions = clf.predict(xtest)\n",
    "    for i in range(396//5):\n",
    "        count = 0\n",
    "        for j in range(1,6):\n",
    "            if(predictions[-(i*5+j)]):\n",
    "                count+=1\n",
    "        true += (count>=counter)==ytest[-(i*5+1)]\n",
    "\n",
    "    print(true/99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "test score 0.8286290322580645\n",
      "real test score 0.6060606060606061\n",
      "---\n",
      "test score 0.8245967741935484\n",
      "real test score 0.696969696969697\n",
      "---\n",
      "test score 0.7701612903225806\n",
      "real test score 0.7575757575757576\n",
      "---\n",
      "test score 0.7399193548387096\n",
      "real test score 0.7575757575757576\n",
      "---\n",
      "test score 0.719758064516129\n",
      "real test score 0.6767676767676768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(X, classe5):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"---\")\n",
    "    xtrain, xtest = X[train_index], X[test_index]\n",
    "    ytrain, ytest = classe5[train_index], classe5[test_index]\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "    clf.fit(xtrain,ytrain)\n",
    "    print(\"test score\",clf.score(xtest,ytest))\n",
    "    true = 0\n",
    "    counter = 5\n",
    "    predictions = clf.predict(xtest)\n",
    "    \n",
    "    for i in range(396//5):\n",
    "        count = 0\n",
    "        for j in range(1,6):\n",
    "            if(predictions[-(i*5+j)]):\n",
    "                count+=1\n",
    "        true += (count>=counter)==ytest[-(i*5+1)]\n",
    "\n",
    "    print(\"real test score\",true/99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok so this approach doesn't seem so good at a first sight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just some insights about articles length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "longueurs = [len(data[i,-1]) for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10018"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(longueurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1523.1834677419354"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(longueurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(longueurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [0 for j in range(101)]\n",
    "x = [j for j in range(101)]\n",
    "for i in range(k):\n",
    "    y[longueurs[i]//100]+=1\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 25, 11, 1, 1, 1]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CONCLUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataBase.csv')\n",
    "del(df['Unnamed: 0'])\n",
    "dfclasse = pd.read_csv('classe.csv')\n",
    "del(dfclasse['Unnamed: 0'])\n",
    "data = np.array(df)\n",
    "classe = np.array(dfclasse)\n",
    "columns = list(df.columns)\n",
    "for i in range(k):\n",
    "    data[i,-1] = stringToList(data[i,-1])\n",
    "    data[i,-2] = stringToList(data[i,-2])\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = []\n",
    "for i in range(k):\n",
    "    X.append(' '.join(data[i,-1]))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.7,min_df=0.01)\n",
    "X = vectorizer.fit_transform(X).todense()\n",
    "\n",
    "pca = PCA(n_components=300)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clementpiat/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/clementpiat/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/clementpiat/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/clementpiat/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "scores=[metrics.accuracy_score,metrics.balanced_accuracy_score,metrics.precision_score,metrics.f1_score,metrics.recall_score]\n",
    "l = len(scores)\n",
    "scores_models = np.zeros((4,l+1),dtype=object)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "models = [model1,model2,model3]\n",
    "\n",
    "for train_index, test_index in skf.split(X, classe):\n",
    "    datatrain, datatest = data[train_index], data[test_index]\n",
    "    xtrain, xtest = X[train_index], X[test_index]\n",
    "    ytrain, ytest = classe[train_index], classe[test_index]\n",
    "    clf = 0\n",
    "    #clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "    clf = LogisticRegression(random_state=1,solver='liblinear',class_weight='balanced',max_iter=1000,C=1)\n",
    "    clf.fit(xtrain,ytrain.ravel())\n",
    "    j=1\n",
    "    for score in scores:\n",
    "        i=0\n",
    "        ypredict = clf.predict(xtest)\n",
    "        scores_models[i,j]+=score(ytest,ypredict)\n",
    "        #print(ytest,ypredict)\n",
    "        for model in models:\n",
    "            i+=1\n",
    "            ypredict = predict(datatest,model)\n",
    "            scores_models[i,j]+=score(ytest,ypredict)\n",
    "        j+=1\n",
    "\n",
    "scores_models /= 5\n",
    "\n",
    "scores_models[0,0]=\"TFIDF-LR\"\n",
    "scores_models[1,0]=\"leadership in title\"\n",
    "scores_models[2,0]=\"leadership in body\"\n",
    "scores_models[3,0]=\"special words in body or title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced accurcy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF-LR</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>0.741473</td>\n",
       "      <td>0.579631</td>\n",
       "      <td>0.611804</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leadership in title</td>\n",
       "      <td>0.762153</td>\n",
       "      <td>0.524333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.089011</td>\n",
       "      <td>0.0486667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leadership in body</td>\n",
       "      <td>0.721542</td>\n",
       "      <td>0.650932</td>\n",
       "      <td>0.452101</td>\n",
       "      <td>0.473815</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>special words in body or title</td>\n",
       "      <td>0.75399</td>\n",
       "      <td>0.651239</td>\n",
       "      <td>0.513711</td>\n",
       "      <td>0.468622</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  accuracy balanced accurcy precision  \\\n",
       "0                        TFIDF-LR    0.7775         0.741473  0.579631   \n",
       "1             leadership in title  0.762153         0.524333       0.6   \n",
       "2              leadership in body  0.721542         0.650932  0.452101   \n",
       "3  special words in body or title   0.75399         0.651239  0.513711   \n",
       "\n",
       "         f1     recall  \n",
       "0  0.611804      0.669  \n",
       "1  0.089011  0.0486667  \n",
       "2  0.473815      0.509  \n",
       "3  0.468622      0.445  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores_models,columns = [\"model\",\"accuracy\",\"balanced accurcy\",\"precision\",\"f1\",\"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==> The TFIDF model seems globally better in terms of precision and F1_score, It's especially good in recall score, meaning that this model is good at identifying positive samples (i.e. good at saying that articles about leadership actually are about leadership)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
